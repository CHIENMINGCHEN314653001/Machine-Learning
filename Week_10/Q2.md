 **CNN 的數學結構與泛化能力，文章內提到 CNN 通過局部連接和權值共享來減少參數量，並使用卷積操作提取特徵**
  * **若從函數逼近論的角度來看，CNN 的結構如何影響它的功能？**
  * **有沒有嚴謹的數學結果適用於 CNN？**

<br></br>
### 已有嚴謹的數學結果

**Ⅰ.證明深度 CNN 在一定架構、濾波器設計下具備「通用逼近性 (universal approximation)」—即對任意連續函數可達到任意精度。**

$$\quad$$ [Universality of Deep Convolutional Neural Networks (D.X. Zhou, 2018)](https://arxiv.org/abs/1805.10769?utm_source=chatgpt.com)


**Ⅱ.在 Sobolev 空間中，給出 CNN 逼近誤差界（non-asymptotic error bounds），特別指出當目標函數支援在近低維流形 (approximate low-dim manifold) 上時，CNN 可減弱維度災難。**

$$\quad$$ [Approximation with CNNs in Sobolev Space (Shen et al., 2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/136302ea7874e2ff96d517f9a8eb0a35-Paper-Conference.pdf?utm_source=chatgpt.com)

**Ⅲ.較早探討卷積網路的逼近能力、結構與參數量關係。**

$$\quad$$ [Approximation Analysis of Convolutional Neural Networks (Bao, 2014)](https://archive.ymsc.tsinghua.edu.cn/pacm_download/654/12351-baocl6.pdf?utm_source=chatgpt.com)

**Ⅳ.探討 CNN 架構在時間序列建模 (Time Series) 上的逼近效率，提出「頻譜基／正則性 (spectrum-based regularity)」與卷積結構的關係。**

$$\quad$$ [Approximation Theory of Convolutional Architectures for Time Series Modelling (Jiang et al., 2021)](https://proceedings.mlr.press/v139/jiang21d/jiang21d.pdf?utm_source=chatgpt.com)

**Ⅴ.給出 CNN 用於非線性算子 (nonlinear operator) 學習時的逼近誤差界，明確點出卷積層深度、濾波器大小、通道數等超參數如何影響誤差。**

$$\quad$$ [Approximation bounds for convolutional neural networks in operator learning (Franco et al., 2023)](https://www.sciencedirect.com/science/article/pii/S0893608023000412?utm_source=chatgpt.com)

<br></br>

**結論**

若從函數逼近論角度，CNN的結構確實影響其逼近能力與泛化潛力，而已有不少嚴謹數學結果支持：包括通用逼近性 (universality)、誤差界 (error bounds)、以及在低維結構情境下的特殊分析。

但是多數結果仍帶強假設（例如目標函數屬於 Sobolev 空間、支援於低維流形、輸入分佈簡化、濾波器／架構受限，對於「真實訓練過程」中如何從隨機初始化、優化、泛化等角度完整說明仍是開放性問題。
