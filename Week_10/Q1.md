* **因 Backpropagation 在連續可微的狀態下執行，若改為「不連續」呢，如何設計有效的 Backpropagation 策略以確保梯度穩定且模型能順利收斂？**
<br></br>
雖然有一些研究提出了方向，但一般深度網路中的不連續函數梯度傳遞如何保證收斂，目前並沒有研究出來。(下方是可能延伸研究的方向)
  * 把原本不連續的地方換成平滑的替代函數，只在反向傳遞用替代函數算梯度，前向仍保留不連續。
  * 直接忽略不連續造成的梯度問題，「假裝」有梯度往回傳，用簡化的梯度近似。
  * 不連續點用「次梯度集合」取代實際梯度，讓反向傳遞仍能執行。
  * 在不連續點附近加入平滑化項，避免梯度爆炸或變成 0。
  * 若不連續場景接近離散決策，可轉成機率形式，透過期望值傳遞梯度。

<br></br> 
* **在眾多 activation function 中，未來會不會存在幾乎完美的激活函數呢?**
<br></br>
理論與實務都顯示不太可能出現真正完美、通用、永遠最佳的激活函數(原因:資料分佈、網路架構、模型深度與寬度、訓練策略與優化器、梯度流動特性與初始化方式等...)；未來的方向更可能是可學習、可自適應、特定場景定制的激活設計，而非找到一個終極唯一答案。

<br></br> 
* **SGD 雖容易計算，但收斂數度慢、不穩定、鞍點問題等，不知道有沒有方法能改善?**
<br></br>
此問題目前沒有單一通用解法，但研究發展出多種改善策略，目標是提高加速度、調步長、加記憶、加噪聲、或加曲率資訊，使模型更快到達好解並避免卡在中間的能力。

改善SGD的主要方法

<tr align = "center">

  | Method  | Faster Convergence | Escape Saddle Point | Stable Training | Weakness |
  | :--- | :---: | :---: | :---: | :--- |
  | Momentum        | ★    | ★    | ★   | sensitive to hyperparameters |
  | Adam            | ★★  | ★    | ★    | weaker generalization        |
  | LR Scheduling   | ★    | –    | ★   | requires tuning              |
  | SWA             | ★    |  ★    | ★   | more compute                 |
  | Noise Injection | –    | ★★   | –    | unstable / hard to tune      |
  | Second-order    | ★★  |  ★   | ★   | expensive                    |

</tr>
