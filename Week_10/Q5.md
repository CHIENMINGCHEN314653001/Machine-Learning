**Logistic regression is convex, but softmax regression (multiclass) can have non-trivial curvature. Under what conditions is the softmax cross-entropy loss strictly convex in parameters?**
<br></br>
**Result:**

Softmax cross-entropy is convex but not strictly convex because the model parameters are not uniquely identifiable: shifting all class weight vectors by the same amount leaves the predictions unchanged, creating flat directions in the loss landscape. Strict convexity emerges only after this ambiguity is removed by imposing an identifiability constraint—such as fixing one class weight vector to zero or requiring all weight vectors to sum to zero—and assuming the data are rich enough to span all relevant feature directions. If L2 regularization is added, the objective becomes strongly convex, ensuring strict convexity regardless of parameter identifiability or data degeneracy.

[Stephen Boyd & Lieven Vandenberghe, Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)<br></br>
[CS229 (Andrew Ng) lecture notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf)<br></br>
[Hastie, Tibshirani, Friedman, The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)<br></br>
[Christopher M. Bishop, Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/prml/)<br></br>
[Kevin P. Murphy, Machine Learning: A Probabilistic Perspective](https://probml.github.io/pml-book/book0.html)
