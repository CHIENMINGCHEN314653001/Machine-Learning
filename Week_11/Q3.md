* Given a smooth activation function $$ğœ$$ , to what extent can a finite-layer neural network approximate the function $$ğ‘¥^ğ‘?$$
* If a neural network can approximate $$ğ‘¥^ğ‘$$ using finite difference methods, can this approximation affect the statistical estimation error of the model parameters $$ğœƒ?$$

<br></br>
**Result**

* **Problem1:**
For any power ğ‘, and over any finite interval $$[-M, M]$$, there exists a neural network with a finite number of layers (or even just one hidden layer) but wide enough, whose output, using a smooth activation function, has an error less than any positive number that we predetermine.
<br></br>
[Multilayer feedforward networks are universal approximators(Hornik, K., Stinchcombe, M., & White, H.,1989)](https://doi.org/10.1016/0893-6080(89)90020-8)<br></br>
[Approximation by superpositions of a sigmoidal function(Cybenko, G.,1989)](https://doi.org/10.1007/BF02551274)<br></br>
[The Power of Depth for Feedforward Neural Networks(Telgarsky, M.,2016)](https://proceedings.mlr.press/v49/telgarsky16.html)

<br></br>
* **Problem2:**
Using the finite difference method to construct neural networks will definitely affect the statistical estimation error of model parameters, but whether this effect is good or bad is not a universally applicable conclusion. It depends on:
<br></br>
  * Problem-relevance: How well does this numerical method match the real-world problem you are trying to solve?
  * Implementation details: How do you integrate the finite difference method into the network architecture?
  * Data quality and quantity: How much data do you have? What is the noise level of the data?
<br></br>
Therefore, this is a open problem, and current research is exploring how to effectively integrate prior knowledge of
numerical analysis into deep learning to achieve better statistical efficiency.
