# 使用神經網路近似 Runge 函數及其導數

## Ⅰ. 介紹
Runge 函數定義如下：

$$
f(x) = \frac{1}{1 + 25x^2}\
$$

在一般討論中的經典測試函數，在邊界附近會出現震盪（**Runge 現象**）。本次目標是訓練神經網路來同時近似 Runge 函數及其導數：

$$
f'(x) = \frac{-50x}{(1+25x^2)^2}.\
$$

---

## Ⅱ. 方法

* 資料生成
  *  區間： $$[-1, 1]$$
  *  樣本數：500
  *  目標函數： $$f(x)$$ 與 $$f'(x)$$

* 神經網路架構
  * 多層感知器 (MLP)
  * 輸入層：1 個神經元
  * 隱藏層：2 層，每層 32 個神經元，Tanh 激活
  * 輸出層：1 個神經元（預測函數值)
  * 導數透過有限差分計算

* 損失函數

$$
L = (1-\alpha) L_{func} + \alpha L_{deriv}
$$

* 其中：
  * $$L_{func}$$：函數值的 MSE
  * $$L_{deriv}$$：導數值的 MSE
  * $$\alpha$$：權重參數

* 訓練細節
  * Adam 學習率 = 0.001
  * Epochs = 800
  * 訓練 / 驗證 / 測試比例 = 70% / 20% / 30%
  * 測試 $$\alpha$$ = 0.0, 0.3, 0.5, 0.7$$


* 隨機種子設定:
  * 由於神經網路的權重初始化與資料切分通常含有隨機性，倘若不固定種子，每次訓練的結果可能會有不同的呈現結果。
  * PyTorch 隨機種子：42
  * NumPy 隨機種子：42

 藉此保證資料、模型以及訓練過程保持一致，確保穩定性與可比對性。

---

## Ⅲ. 結果

* 函數與導數近似
  * 函數值：擬合效果良好
  * 導數：準確度依賴 $$\alpha$$ ，適當權重能顯著改善

* 損失曲線
  * 設定下均穩定收斂
  * $$\alpha$$ = 0.5 在兩者之間取得最佳平衡

* 誤差比較

| $$\alpha$$ | 函數 MSE | 函數最大誤差 | 導數 MSE | 導數最大誤差 |
|-------|----------|--------------|----------|--------------|
| 0.0   | ~1e-4    | ~1e-2        | 高       | 高           |
| 0.3   | 更低     | 更低         | 改善     | 改善         |
| 0.5   | **最佳** | 小           | 小       | 小           |
| 0.7   | 上升     | 較大         | 降低     | 較小         |

---

## Ⅳ. 觀察
- 僅用函數損失 ($$\alpha=0$$) : 函數擬合好，但導數差
- 加入導數損失 ($$\alpha>0$$) : 導數改善，函數稍降
- $$\alpha=0.5$$ → 最佳平衡
- 邊界誤差較大，可透過以下方式改善：
  - 更深/更寬網路
  - 邊界增加取樣

---

## Ⅴ. 結論
神經網路能有效的近似 Runge 函數以及其導數的部分，損失權重 $$\alpha$$ 太小會導致斜率估計較差，而  $$\alpha$$ 太大會降低函數擬合。 由作圖可看出 $$\alpha=0.5$$是最好的選擇。
進而也可知合理損失函數的重要性。
- 神經網路可同時近似函數與導數
- $$\alpha$$ 權重是關鍵
- $$\alpha = 0.5$$ 效果最佳
  - 函數 MSE ≈ $$10^{-4}$$
  - 導數 MSE ≈ $$10^{-3}$$

---
